\newpage

\chapter{Conclusions}
\label{chap:conclusions}

In this dissertation we proposed a framework for designing splitting criteria for handling multi-valued nominal attributes. Criteria derived from our framework can be implemented to run in polynomial time in $n$ and $k$, with theoretical guarantee of producing a split that is close to the optimal one. We also made an experimental study on criteria based on different heuristics, some of them with approximation guarantees (Hypercube Cover and Largest Class Alone), some of them without one (PC-ext). We compared their ability of finding splits with the lowest impurity and, later, compared them with Twoing and criteria from our framework to analyse the accuracy of trees obtained by using them.

Experiments over 11 datasets suggest that the GL$\chi^2$ criterion, obtained from our framework, is competitive with the well-established Twoing criterion in terms of both accuracy and speed for datasets with a small number of classes ($k \leq 7$). It is also much faster than Twoing when the number of classes is greater than 10, while keeping a comparable accuracy. Overall, Hypercube Cover also had results similar to Twoing. Therefore, our methods are an interesting alternative to deal with datasets with a large number of classes that contain nominal attributes with a large number of different values, since those cannot be properly handled by Twoing due to its exponential running time dependence on the number of classes.

Even though the PC-ext criterion does not have a theoretical guarantee, the experiments also show that it has some advantage in terms of accuracy over the other methods, except when used in the conditional inference tree framework. This suggests that PC-ext is very good in terms of comparing different attributes among themselves, but not as good in finding the best split for a given attribute. In these bias-free experiments, the GL$\chi^2$ criterion had the best results. In terms of speed, it also has an advantage over every other criterion, except for LCA.

Although we discovered, in Chapter \ref{chap:experiments-splits}, that the Largest Class Alone heuristic obtains splits with worse impurity than Hypercube Cover and PC-ext, we saw in Chapter \ref{chap:experiments-datasets} that trees created using it have competitive accuracy. This suggests one should use it when the dataset is big enough for the training time to be a concern.

In practice, one should probably use PC-ext to train decision trees when its training time is acceptable, otherwise one should switch to Largest Class Alone. Furthermore, when using the Conditional Inference Tree framework, the best splitting criterion to use is GL-$\chi^2$. Its running time is polynomial---in general dominated by the framework itself---and obtains trees with the best accuracies. This indicates that, given an attribute, it is the best at choosing splits.

Lastly, our experiments also reinforce the potential of aggregating attributes as a tool for improving the accuracy of decision trees. An interesting topic for  future research is evaluating the behavior of our criteria in boosted tree methods. Another direction for future work is developing new methods for automatic aggregating attributes, or improving the available ones.
