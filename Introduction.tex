\newpage

\chapter{Introduction}
\label{chap:introduction}

Decision Trees and Random Forests are among the most popular 
methods for classification tasks. Decision Trees, specially small ones, are easy to interpret,
while Random Forests usually yield more accurate classifications. One of the key issues in these methods
is how to select an attribute to associate with a node of the tree/forest. An important
related issue is how to split the samples once the attribute is selected.

There is a number of papers  discussing aspects related with 
attribute selection, such as:
how to design criteria to evaluate the quality of different types of attributes;
whether binary or multi-way splits shall be used and
how to remove bias from  splitting criteria.
For recent surveys on this topic we refer to \cite{books/sp/datamining2005/RokachM05},
\cite{Loh2014} and \cite{series/sbcs/BarrosCF15}.

Many criteria, with different properties,  have been proposed to evaluate 
the quality of different types of attributes, including
continuous and categorical ones.  Among the most popular criteria,
we have the Gini Gain and the Information Gain (\cite{Breiman84}, \cite{quinlan2014c4}).

Despite the large body of work we believe  there are still questions to be answered.
One of them is to how to  properly handle nominal  attributes that may assume a large number of values.
Before explaining the reason behind our  statement we would
like to remark that this kind of attribute
appears naturally in some applications  (e.g.: states of a country or letters from some alphabet).
In addition, they may arise as the result of aggregating
attributes that have few distinct values
with the goal of capturing possible correlation between them, as pointed out by \cite{Chou:91}.
As an example, consider 5 binary attributes (e.g. medical tests) and a
target binary variable  that has large probability of being positive if at least $3$ out
of the $5$ binary tests are positives. By aggregating
the $5$ binary variables we obtain a new attribute with $2^5=32$
values that  captures  this relation. 
If we used the 5 attributes separately we would 
need 5 levels in the tree to be able to capture the relation between
them and the target class, thus 
incurring a large fragmentation of the set of samples.

To properly face  multi-valued nominal attributes we have to deal with the computational time required to compute good splits.
Our contribution, explained in the next section, is related with this issue. 
 
A brute force search to compute the best binary split 
requires $\Omega(2^n)$ time, where $n$ is the number of distinct values the attribute may assume. The computational efficiency can be improved if a $n$-ary split is used rather than a binary one. However,  this may lead to a severe fragmentation of the sample space, which is not desirable: the number of samples available for each of the children of the split node 
may be small and, as a consequence, the underlying classification tasks may become significantly more difficult.
When the target variable is binary, a family of impurity measures that include both the Gini Gain and the Information Gain can be computed efficiently, as shown
in the influential monograph by Breiman et al \cite{Breiman84}.
However, when the number of classes $k$ is larger than 2,
most, if not all, of the available exact solutions take exponential time
in $(n,k)$.
The Twoing method, also from \cite{Breiman84}, 
is an  interesting case since its running time is $O(2^{\min\{n,k\}})$ rather than $O( 2^ n)$ while being equivalent to
Gini Gain when $k=2$.

When both $n$ and $k$ are large, in the sense that an exhaustive search does not run in a reasonable time, one can rely on heuristics to compute the best binary split.
As an example, the GUIDE algorithm  \cite{Loh2009}, the  last
of a series of algorithms/developments designed by Loh and its contributors, 
deals with a nominal variable $X$
as follows: if $k=2$ or $n \le 11$ the Gini Index is computed;
if $k \le 11$ and $n > 20$ a new variable $X'$ with at most $k$ distinct values is created according to a certain rule and an exhaustive search is performed over it;  finally, if $k > 11$ or $n \le 20$,  $X$ is binarized and a Linear Discriminant Analysis (LDA) is employed.
These rules reflect the difficulty in dealing with multi-valued nominal attributes. Other interesting heuristics are the PC and PC-ext criteria, which calculate the principal component of the class probability vectors and uses the order given by the vector projections in this direction to look for splits.
In general, the main drawback of using heuristics is the lack of a theoretical guarantee about their behavior. 


\section{Our Contribution}
\label{sec:contribution}

Given this scenario,  in chapter \ref{chap:framework} we propose
a framework for designing criteria, with nice theoretical properties, for evaluating the quality of 
 multi-valued nominal attributes.
Criteria generated according to this framework
run in polynomial time in both the number of values and classes and
have a theoretical guarantee that they are close to optimal.
The key idea consists of formulating the problem
of finding the best binary partition for a given attribute $A$ as the  problem of finding a 
cut with maximum weight  in a complete graph whose nodes are associated with the values that $A$ may assume and the edges' weights capture the benefit of putting
values in different partitions. The  motivation behind the use of the max-cut problem is 
the existence of efficient algorithms with 
approximation guarantee, in particular 
the one proposed  by \cite{GoeWil95}, with $0.878$ approximation,
and  local search  algorithms with 0.5-approximation as shown in \cite{journals/corr/AngelBPW16}.


We discuss two criteria that are derived from this framework:
the first one  can be seen as a natural variation of the
Gini Gain, while the second criterion uses the $\chi^2$-test  to set the edges' weights. For that, each
edge $e_{ij}$ between nodes  $v_i$ and $v_j$
is thought as a binary attribute $A(i,j)$ with values $v_i$ and $v_j$.
After discussing these criteria, we show how to extend them to handle
numeric attributes.

We also  present a number of experiments that suggest that one of our criteria is competitive with the Twoing method, which is -- as far as we know -- the only well-established criterion with binary splits that can be optimally computed for large $n$ when $k > 2$. However, in contrast with our methods, Twoing cannot handle datasets that also have a large number of classes. Some criteria based on heuristics, such as the PC-ext and Hypercube Cover, are also part of the comparison. In addition, the experiments also  provide evidence of the potential of aggregating  attributes for improving the accuracy of decision trees.

\section{Related Work}
\label{chap:relatedwork}

Many splitting criteria have been proposed to 
deal with continuous and nominal attributes.
Arguably, the Gini Gain---used by CART---and entropy-based measures---such as 
the Information Gain, adopted by C4.5---are among
the most popular (\cite{books/sp/datamining2005/RokachM05,
Loh2014,series/sbcs/BarrosCF15}).

There has been  some investigation on 
methods to compute the best split efficiently 
(\cite{Breiman84,Chou:91,BPKN:92,journals/datamine/CoppersmithHH99}).
For the 2-class problem,  \cite{Breiman84} proved a theorem which states that an optimal
binary partition, for a certain class of splitting criteria,
can be determined in linear time on $n$ (TODO: qual o erro?), the number of distinct values of the attributes, after ordering.
The Gini  Gain belongs to this class.
The  other  three papers generalize
this theorem  in different directions 
and show necessary conditions that are satisfied by optimal partitions for a certain class of splitting criteria. 
These conditions, though useful to restrict the set of partitions
that need to be considered, do not yield  a method that
is efficient (polynomial time) for large values of $n$ and $k$. These papers also  present  heuristics, without approximation guarantee, to obtain good splits.
Another related result is a theorem from \cite{journals/datamine/CoppersmithHH99} that guarantees that the optimum split can be found by separating the class probability vectors by a hyperplane. This motivated the creation of the PC criterion, as will be shown in the next chapter. TODO: falar do Sliq e FlipFlop.

Other proposals to  speed up the attribute selection phase
 include  \cite{MolaSiciliano1997,Shih2001}. 
The first presents a simple  heuristic
to reduce the number of binary splits considered to
choose the best nominal variable among the $m$ available ones.
 The second   extends the method for another class
of impurity measures.

In order to properly
handle nominal attributes with a large number of values,
apart from efficiently computing good splits, it is
important to prevent bias in the attribute selection.
Indeed, it is widely  known that many splitting criteria have bias toward
attributes with a large number of values. There are some  proposals available
to cope with this issue 
(\cite{conf/icml/DobraG01,Shih2004,Hothorn:2006:URP}). 
This topic, though relevant, is not the focus of this dissertation.

\section{Organization}
\label{sec:organization}
In chapter \ref{chap:background} we explain how decision trees are used for classification problems and how they are constructed. We also present the main impurity measures and splitting criteria used in the literature, together with their execution-time complexity.

Chapter \ref{chap:framework} contains the framework for generating splitting criteria that run in polynomial time. Its relation with the Max-Cut problem and its approximation algorithms are explained and some criteria obtained from this framework are presented.

Later, in chapters \ref{chap:experiments-splits} and \ref{chap:experiments-datasets}, we compare the criteria and see how they perform in practice. In chapter \ref{chap:experiments-splits} we explore how the many heuristics used to find splits with optimal impurity compare among themselves. This suggests a couple of criteria that perform better and can be used when the number of values and classes are large. In chapter \ref{chap:experiments-datasets} we analyze these methods on real datasets that contain attributes with large number of values and classes. Lastly, in chapter \ref{chap:conclusions} we present our study conclusions.