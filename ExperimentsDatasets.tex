\newpage

\chapter{Experiments on Real Datasets}
\label{chap:experiments-datasets}


In this chapter we describe our experimental study on real datasets. First, we describe the chosen datasets. Next, we discuss the max-cut algorithms 
employed and, then, we present our results. We will compare the performance between Twoing, Hypercube Cover, PC-ext and some criteria generated from our framework. Both Hypercube Cover and PC-ext were chosen based on the experiments results of the previous chapter.

All  experiments described in the following sections were executed on a machine with the following settings: Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz with 32 GB of RAM. The code was developed using Python 3.6.1 with the libraries numpy, scipy, scikit-learn and cvxpy.
The project can be accessed in {\tt github.com/felipeamp/dissertation-code}.

TODO: tornar repositorio publico.

\section{Datasets}
We employed 11 datasets in total. Eight of them are from the UCI repository:
Mushroom, KDD98, Adult, Nursery, Covertype, Cars, Contraceptive and Poker (\cite{Lichman:2013}).
Two others are available in Kaggle: San Francisco Crime and Shelter Animal Outcome
\cite{SFC,AnimalShelter}. The last dataset was created by translating texts from the Reuters database \cite{Lichman:2013} into phonemes, using the CMU pronouncing dictionary (see \cite{CMU-PD}).

We chose  these datasets  because they
have at least 1000 samples and they  either contain  multi-valued attributes 
or attributes that can be naturally aggregated to produce multi-valued attributes. 
From the KDD98 dataset we derived the datasets
KDD98-k, for $k = 2$ and $9$. These datasets contain
only the positive samples (people that donate money) 
of KDD98 and the target attribute, Target$\_$D, is split into $k$ classes, where the $i$-th
class correspond to the $i$-th quantile in terms of amount of money donated. For the Reuters Phonemes dataset,
we extracted 10000 samples containing the 15 most common phonemes as class and try to predict when they are about to happen given the 3 preceding phonemes.
This dataset is motivated by Spoken Language Recognition problems, where phonotactic models are used as an important part of the classification system, as seen in \cite{conf/interspeech/Navratil06}. 
For the San Francisco Crime dataset, we try to predict the crime category given the month, day of the week, police department district and latitute/longitude. Lastly, for the Shelter Animal Outcomes dataset, we converted the age into a numeric field containing the
number of days old and separated the breed into two categorical fields, repeating the breed in both in case there was only one originally. We also removed the AnimalID, Name and the DateTime. For this dataset we try to predict the outcome type and subtype (concatenated into a single categorical field). For both 
San Francisco Crime and  Shelter Animal Outcomes
 datasets we created a version of them ({\tt S.F. Crime-15} and {\tt Shelter-15}), containing only 15 classes, instead of the 39 and 22 original ones, respectively. This was done by grouping the rarest classes into a single one. 

\begin{table}
\centering
\begin{tabular}{c|c|c|c|c|c}
Dataset             & Samples  &  k        & $m_{nom}$ &  $m^{ext}_{nom}$ &   $m_{num}$   \\  \hline
{\tt Mushroom}      & 5644     & 2         & 22        & N/A              & 0             \\ 
{\tt Adult}         & 30162    & 2         & 8         & N/A              & 6             \\
{\tt KDD98}         & 4843     & {\tt Reg} & 65        & N/A              & 314           \\ 
{\tt Nursery}       & 12960    & 5         & 8         & 11               & 0             \\ 
{\tt CoverType}     & 581012   & 7         & 44        & 46               & 10            \\ 
{\tt Car}           & 1728     & 4         & 6         & 8                & 0             \\ 
{\tt Contracep}     & 1473     & 3         & 7         & 9                & 2             \\ 
{\tt Poker}         & 25010    & 10        & 10        & 0                & 0             \\
%{\tt Shelter-15}    & 26711    & 15        & 5         & N/A              & 1             \\
{\tt Shelter}       & 26711    & 22        & 5         & N/A              & 1             \\      
%{\tt S.F. Crime-15} & 878049   & 15        & 3         & N/A              & 2             \\      
{\tt S.F. Crime}    & 878049   & 39        & 3         & N/A              & 2             \\  
{\tt Phonemes}      & 10000    & 15        & 3         & N/A              & 0 
\normalsize
\end{tabular}
\caption{Information about  the employed datasets after data cleaning and attributes aggregation.
Column $k$ is the number of classes and {\tt Reg} stands
for Regression; columns $m_{nom}$ and $m^{ext}_{nom}$ are the
number of  nominal attributes in the original and the
extended datasets (when it exists), respectively; column $m_{num}$ is the number of  numeric attributes.}
\label{exp:datasets}

\end{table}



We also created extended versions
of some of the above datasets 
by adding nominal attributes  obtained by aggregating some of the original ones, as 
we detail below.  
Our goals are  examining the impact of multi-valued
attributes in the classification performance and 
also understanding how the  different 
splitting criteria  handle them.


\begin{table}[]
\centering
\begin{tabular}{c|c|c} 
{\tt parents}     & {\tt has\_nurs}    & {\tt Aggregated Attribute}   \\ \hline
usual       & proper       & usual-proper             \\
usual       & less\_proper & usual-less\_proper       \\
usual       & improper     & usual-improper           \\
usual       & critical     & usual-critical           \\
usual       & very\_crit   & usual-very\_crit         \\
pretentious & proper       & pretentious-proper       \\
pretentious & less\_proper & pretentious-less\_proper \\
pretentious & improper     & pretentious-improper     \\
pretentious & critical     & pretentious-critical     \\
pretentious & very\_crit   & pretentious-very\_crit   \\
great\_pret & proper       & great\_pret-proper       \\
great\_pret & less\_proper & great\_pret-less\_proper \\
great\_pret & improper     & great\_pret-improper     \\
great\_pret & critical     & great\_pret-critical     \\
great\_pret & very\_crit   & great\_pret-very\_crit  
\end{tabular}
\caption{Aggregation of attributes parents and has\_nurse from dataset {\tt Nursery}.}
\label{tab:Aggreation}

\end{table}

Table \ref{tab:Aggreation} illustrates this construction.

\begin{itemize}

\item {\tt Nursery-Ext}. This dataset is obtained by adding three
new attributes to dataset Nursery. 
The first attribute has  $15$ distinct values
and it is constructed through the  aggregation of   2 attributes 
from group  EMPLOY, one with 5 values and the other with 3 values. 
The second attribute has 72 distinct values
corresponding to the aggregation of attributes from 
the attributes in group {\tt STRUCT\_FINAN}.
The third attribute, with 9 distinct values, is the combination of
the  attributes in group {\tt SOC\_HEALTH}.


\item {\tt Covertype-Ext}. 
We combined 40 binary attributes
related with the soil type  into a new attribute with 40 distinct values.
The same approach was employed to combine the 4 binary attributes related
with the wilderness area into a new attribute
with 4 distinct values.
This is an interesting case because, apparently, the 40 (soil type)
binary attributes  as well as the 4 (wilderness area) binary attributes
 were derived from a binarization of two attributes, one with 40 distinct value and the other with 4 distinct values.
 

\item {\tt Cars-Ext}. To obtain this dataset, the 2 attributes
related with the concept {\tt PRICE},  {\tt buying} and {\tt maint},
were combined into an attribute
with 16 distinct values.
Moreover, the 3 attributes
related with concept {\tt CONFORT} were  combined into an 
an attribute with 36 distinct values.

\item {\tt Contraceptive-Ext}. The 2 attributes
related with the couple's education were combined into an
attribute with 16 distinct values.
Moreover, the 3 attributes related with the couple's occupations  and  standard of living
were  aggregated into a new attribute with 32 distinct values.

\end{itemize}

Samples with missing values were removed from the datasets.
Table \ref{exp:datasets} provides some statistics.




\section{Computing the Maximum Cut}
 

The GW algorithm  requires  the solution of a semidefinite program (SDP),
which may be computationally expensive despite  its 
polynomial time worst case  behavior.
As an example, for an attribute with 100 distinct values, the solution of
the corresponding SDP takes in average about 2 second in our machine.
One the one hand, this is a tiny amount of time 
compared with that required to perform an exhaustive search on the $2^{100}$
possible binary partitions. On the other hand, 
 faster alternatives are desirable, even
at the cost of losing part of the theoretical approximation guarantee. 

To avoid solving a SDP, 
we also evaluated a procedure
that first executes the {\tt GreedyCut} algorithm presented
in Section \ref{sec:maxcutbackground} and then runs a local search as described
in the Algorithm  \ref{alg:localsearch} in the same section. 
The use of this approach combined with the two
ways of setting the edges' weights lead to
 Greedy LocalSearch SquaredGini (GLSG) and 
Greedy LocalSearch $\chi^2$ (GL$\chi^2$) 
criteria, respectively.
For attributes with 100 distinct values this approach is 60-70
times faster than the one based on the GW algorithm.


\section{Experimental Results}

We performed a number of experiments to evaluate how the
proposed methods behave with real datasets.
All experiments consist of building decision trees
with a predefined maximum depth.  In addition,
to prevent the selection of non-informative nominal attributes, 
we used a $\chi^2$-test for each attribute at every node of
the tree: if the $\chi^2$-test  on the contingency table of attribute $A$
has $p$-value larger than $10\%$ at a node $\nu$, then
$A$ is not used in $\nu$. Furthermore, attributes with less than 15 samples associated with its
second most frequent value are also not considered for splitting. This helps  avoid data overfitting.


\subsection{Maximum Depth 5}

Table \ref{tab:nominal-5} presents  the results of an experiment to
compare the accuracy of  Decision Trees built by  our methods with those built by Twoing, Hypercube Cover and PC-ext.
In this experiment, the maximum depth was set to 5 and we considered just the nominal attributes of the datasets. 
The motivation for this depth is to produce trees that
are relatively easy to interpret and, in addition, some random
forests methods such as boosting work with shallow trees.
Each accuracy is the average of 20 stratified 3-fold cross-validations,
each generated with a different seed.
The entry  associated with  $({\cal D},I)$ has two pieces of information: the average accuracy
of criterion $I$ on dataset ${\cal D}$ and the number of criteria
with accuracy   statistically lower than that of $I$ on dataset ${\cal D}$. 
The statistical test used for criteria comparison is a  one-tailed paired $t$-student test with a $95\% $ confidence level. 
In general, there was a clear advantage towards PC-ext and a clear disadvantage towards the GW-based criteria. Twoing, Hypercube Cover and the GL criteria had somewhat similar results. The advantage of the GL-based criteria over the Gw-based ones is likely related with the fact that the weights of the cuts computed by the GL approach in this experiment are, in general, larger than those obtained by the GW algorithm.


\begin{sidewaystable*}[ph!]
\centering
\begin{tabular}{c|cc|cc|cc|cc|cc|cc|cc} 
Dataset & \multicolumn{2}{c|}{Twoing} &  \multicolumn{2}{c|}{GWSG}  
&   \multicolumn{2}{c|}{GW$\chi^2$}                   &\multicolumn{2}{c|}{GLSG}       &\multicolumn{2}{c|}{GL$\chi^2$} & \multicolumn{2}{c|}{PC-ext} & \multicolumn{2}{c}{HcC}\\  \hline 
% Dataset           &        Twoing     &     GWSG          &      GW$\chi^2$   &       GLSG        &    GL$\chi^2$     &     PC-ext                 &         HcC
{\tt Adult}         & 82.21    & (2)    & 81.91    & (1)    & 82.24    & (2)    & 81.83    & (0)    & 82.24    & (2)    &{\bf82.31}&{\bf(5)}         & 82.21    &    \\
{\tt Mushroom}      & {\bf 100}&{\bf(2)}& 99.99    & (0)    & 99.98    & (0)    &{\bf  100}&{\bf(2)}& 99.99    & (0)    &{\bf 100} &{\bf(2)}         &{\bf100}  &    \\
{\tt KDD98}-2       & 80.47    & (0)    & 81       & (2)    & 80.74    & (1)    & 81.16    & (3)    & 80.51    & (0)    &{\bf81.25}&{\bf(4)}         & 80.47    &    \\
%{\tt KDD98}-3       & 63.77    & (0)    & 63.8     & (0)    & 64.37    &{\bf(3)}& 63.75    & (0)    &{\bf64.54}&{\bf(3)}&          &                 &          &    \\
%{\tt KDD98}-5       & 48.02    & (2)    & 46.79    & (0)    &{\bf48.59}&{\bf(3)}& 46.77    & (0)    & 48.58    &{\bf(3)}&          &                 &          &    \\
{\tt KDD98}-9       & 40.35    & (2)    & 38.13    & (0)    & 40.93    &{\bf(4)}& 38.15    & (0)    &{\bf 41 } &{\bf(4)}& 40.27    & (2)             & 40.14    &    \\
{\tt Nursery}       & 88.25    &{\bf(3)}& 88.03    & (0)    & 88.2     & (0)    &{\bf88.33}&{\bf(3)}& 88.2     & (0)    & 88.25    &{\bf(3)}         & 88.25    &    \\
{\tt Nursery-Ext}   &{\bf93.82}&{\bf(4)}& 90.95    & (0)    & 93.02    & (2)    & 90.75    & (0)    & 93.13    & (2)    & 93.81    &{\bf(4)}         & 93.81    &    \\
{\tt Cars}          & 86.53    & (1)    & 85.39    & (0)    & 86.37    & (1)    &{\bf87.93}&{\bf(5)}& 86.42    & (1)    & 86.5     & (1)             & 86.5     &    \\
{\tt Cars-Ext}      & 90.3     & (0)    & 90.82    & (2)    & 91.57    & (4)    & 90.84    & (2)    &{\bf 91.9}&{\bf(5)}& 90.32    & (0)             & 90.32    &    \\
{\tt Contracep}     & 43.77    & (0)    & 43.96    & (0)    &{\bf44.04}&{\bf(1)}& 43.89    & (0)    & 44       & (0)    & 43.59    & (0)             & 43.62    &    \\
{\tt Contracep-Ext} & 43.17    & (0)    & 44.32    &{\bf(4)}& 43.44    & (0)    &{\bf44.35}&{\bf(4)}& 43.7     & (0)    & 43.77    & (1)             & 43.36    &    \\
{\tt CoverType}     & 52.97    & (2)    & 55.05    & (3)    & 51.07    & (0)    & 55.05    & (3)    & 51.07    & (0)    &{\bf58.12}&{\bf(5)}         &{\bf58.12}&    \\
{\tt CoverType-Ext} & 64.48    & (4)    & 64.12    & (2)    & 57.73    & (0)    & 64.23    & (3)    & 59.95    & (1)    &{\bf64.71}&{\bf(5)}         & 64.54    &    \\ 
{\tt Poker }        & 51.9     & (3)    & 50.28    & (1)    & 51.77    & (2)    & 49.94    & (0)    &{\bf51.91}&{\bf(4)}& 51.7     & (2)             & 51.69    &    \\
{\tt Shelter-15}    & 48.01    & (3)    & ---      & (0)    & ---      & (0)    & 45.31    & (2)    &{\bf48.13}&{\bf(4)}& 48.07    & (3)             & 48.05    &    \\   
{\tt S.F. Crime-15} &{\bf22.1} &{\bf(2)}& 21.32    & (1)    & 22.09    &{\bf(2)}& 21.23    & (0)    & 22.09    &{\bf(2)}& 22.09    &{\bf(2)}         & 22.09    &    \\
{\tt Phonemes}      &{\bf30.92}&{\bf(5)}& ---      & (0)    & ---      & (0)    & 30.29    & (3)    & 29.47    & (2)    & 30.59    & (4)             & 29.92    &    \\
\hline
Average (Sum)       &     63.7 & (33)   & 66.81*   & (16)   & 66.66*   & (19)   & 63.33    & (30)   & 63.36    & (27)   & 64.08    & (43)            & 63.94    & ()

\end{tabular}
\caption{Average accuracy and statistical tests  for  decision trees 
with depth at most 5 using only nominal attributes. The best accuracy for each dataset is bold-faced. Experiments that did not finish in reasonable time are considered statistically worse than the others. These criteria have a * mark besides their average accuracies, since they are calculated only on the experiments that finished.}
\label{tab:nominal-5}
\normalsize
\end{sidewaystable*}


The results of  Table \ref{tab:nominal-5} also
provide evidence of  the potential
of considering aggregated attributes. 
The accuracy obtained for the extended versions of datasets
{\tt Nursery}, {\tt Cars} and {\tt CoverType} are considerably higher than those obtained for 
the original versions. For {\tt Contracep}, the effect is not clear.

Another key aspect to discuss is the computational cost of the proposed criteria. Table \ref{tab:time-5} shows the running time of each criterion in the experiment of Table \ref{tab:nominal-5}. Twoing is the fastest method when the number of classes is small and the GL-based methods  and PC-ext become competitive and eventually the
fastest ones when the number of classes gets larger. As the number of classes increases, the GL-based methods and PC-ext become much faster than Twoing, with the turning point being around $k=7$. For datasets with 15 classes our GL criteria are 15-300 times faster. The same can be seen for PC-ext, which is 15-600 times faster. We also ran experiments using all the classes available in both the {\tt S.F. Crime} and {\tt Shelter} datasets (39 and 22, respectively). Twoing and the GW criteria can not be executed in a reasonable time with that many classes, while GLSG and GL$\chi^2$ ran in approximately 100 seconds on the {\tt S.F. Crime} dataset and  300 seconds on the {\tt Shelter} dataset (PC-ext ran in 75 and 32 seconds, respectively). This behavior for the Twoing criterion is not surprising, since its running time has an exponential dependence of the number of classes $k$. Nonetheless, since the execution time for our criteria in this experiment grew in an approximately linear fashion with $k$, it suggests that they can also be used with datasets that have a much larger number of classes. It is also interesting to note that the aggregated attributes usually appeared at or near the root of the decision trees. Lastly, the running time for the GW-based criteria were usually one or two orders of magnitude larger than the others. The only clear exception was in the CoverType dataset, where the number of samples is very large while the attributes’ number of values is much smaller. Hypercube Cover behaves very similarly to the Twoing criterion, but it is a bit slower because the impurity calculation with $k$ classes is slower than with $2$ classes.

Since the GW-criteria performe much worse in terms of execution time and yield comparable or worse accuracy than the other criteria, they were not used in any of the experiments that follow.

\begin{table*}[]
\small
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c}
Dataset             & k  & Twoing     & GWSG  & GW$\chi^2$  & GLSG      & GL$\chi^2$ & PC-ext     & HcC        \\ \hline
{\tt Adult}         & 2  & {\bf  2.7} & 41    & 88          & 3         & 4          & 2.8        & 3.7        \\
{\tt Mushroom}      & 2  & {\bf 0.6}  & 6.9   & 8.6         & 0.9       & 1          & 0.7        & 0.9        \\
{\tt KDD98}-2       & 2  & {\bf 4}    & 2162  & 3579        & 44        & 44         & 5.2        & 5.8        \\
{\tt Contracep}     & 3  & {\bf 0.1}  & 0.6   & 1           & 0.1       & 0.1        & 0.1        & 0.1        \\
{\tt Contracep-Ext} & 3  & {\bf 0.1}  & 3.3   & 13          & 0.2       & 0.2        & 0.2        & 0.2        \\
%{\tt KDD98}-3       & 3  & {\bf 5.2}  & 2289  & 4342        & 60        & 56         &            &            \\
{\tt Cars}          & 4  & {\bf 0.1}  & 2.5   & 2.4         & 0.1       & 0.1        & 0.1        & 0.2        \\
{\tt Cars-Ext}      & 4  & 0.2        & 7.7   & 11          & 0.3       & 0.3        & {\bf 0.2}  & 0.4        \\
{\tt Nursery}       & 5  & {\bf 0.8}  & 4.7   & 5           & 1         & 0.9        & 0.9        & 1.2        \\
{\tt Nursery-Ext}   & 5  & {\bf 1.1}  & 76    & 148         & 3.3       & 2.6        & 1.4        & 1.7        \\
%{\tt KDD98}-5       & 5  & {\bf 11}   & 2469  & 4956        & 81        & 63         &            &            \\
{\tt CoverType}     & 7  & 349        & 265   & {\bf 179}   & 245       & 308        & 271.4      & 338        \\
{\tt CoverType-Ext} & 7  & 213        & 341   & 213         & {\bf 182} & 296        & 196.5      & 259        \\
{\tt KDD98}-9       & 9  & 132        & 3410  & 5899        & 97        & 74         & {\bf 14.7 }& 298        \\ 
{\tt Poker}         & 10 & 7.4        & 6.5   & 11.2        & 2.2       & {\bf 2.1}  & 2.5        & 13.3       \\
{\tt Shelter-15}    & 15 & 3599       & ---   & ---         & 149.9     & 166.3      & {\bf 14.1 }& 7968       \\   
{\tt S.F. Crime-15} & 15 & 638.2      & 605.2 & 823.7       & 40.7      & {\bf 40.2} & 41.7       & 1224       \\ 
{\tt Phonemes}      & 15 & 1343       & ---   & ---         & 4.4       & 5.5        & {\bf 2.2 } & 2187
\end{tabular}
\caption{Average time in seconds of a 3-fold cross validation for building decision trees with depth at most 5. The fastest method for each dataset is bold faced.}
\label{tab:time-5}
\end{table*}



Table \ref{tab:ctree-5} presents the 
comparison between our GL methods, Twoing, Hypercube Cover and PC-ext in another scenario,
where we use $c_{quad}$, one of the bias-free criterion proposed in \cite{Hothorn:2006:URP}, to select the attribute at each node of the tree. 
Then, both Twoing, Hypercube Cover, PC-ext and our methods are  used only for splitting the chosen attribute, which allows for a  more direct comparison of their splitting ability. Once again we observed a balance between the different criteria, with an advantage towards GL-$\chi^2$. Perhaps surprisingly, the bias free approach had significantly worse results for the datasets with extended attributes. Another surprise was that the PC-ext criterion behaved much more poorly than in the previous experiment. This suggests that its advantage lies in comparing different attributes, and not necessarily finding the best split. This experiment also showed that it is not possible to run $c_{quad}$ in reasonable time for the {\tt Shelter-15} dataset. This happens because it calculates a pseudoinverse of a matrix whose dimension grows with the number of values and classes, which is infeasible for large $n$ and $k$.


\begin{table*}
\small
\centering
\begin{tabular}{c|cc|cc|cc|cc|cc} 
Dataset  &   \multicolumn{2}{c|}{CI-Twoing} &   \multicolumn{2}{c|}{CI-GLSG} & \multicolumn{2}{c|}{CI-GL$\chi^2$}& \multicolumn{2}{c|}{CI-PC-ext}& \multicolumn{2}{c}{CI-HcC} \\  \hline   
% Dataset          &        CI-Twoing       &        CI-GLSG          &      CI-GL$\chi^2$      &       CI-PC-ext         &       CI-HcC
{\tt Adult}        &{\bf 81.96} &{\bf  (3)} & 81.61       & (1)       & 81.77       & (2)       & 79.98       & (0)       & {\bf 81.96} &           \\
{\tt Mushroom}     &86.97       & (1)       &{\bf  94.79 }& {\bf (3)} & 90.15       & (2)       & 86.77       & (0)       & 86.97       &           \\
{\tt KDD98}-2      &81.29       & (0)       & {\bf 82.34 }& {\bf (3) }& 81.68       & (2)       & 81.35       & (0)       & 81.29       &           \\
%{\tt KDD98}-3      &65.28       & (0)       & {\bf 65.8  }& {\bf (2) }& 65.07       & (0)       &             &           &             &           \\
%{\tt KDD98}-5      &49.23       & (0)       & {\bf 49.58} & {\bf (2)} & 49.27       & (0)       &             &           &             &           \\
{\tt KDD98}-9      &41.84       & (0)       & 42          & (0)       & {\bf 42.26} & {\bf (2)} & 41.73       & (0)       & 41.95       &           \\
{\tt Nursery}      &{\bf 88.48} & {\bf (1)} & 88.3        & (0)       & 88.47       & {\bf (1)} &{\bf 88.48 } & {\bf (1)} & {\bf 88.48} &           \\
{\tt Nursery-Ext}  &{\bf 88.48} & {\bf (1)} & 88.26       & (0)       & 88.47       & {\bf (1)} &{\bf 88.48 } & {\bf (1)} & {\bf 88.48} &           \\
{\tt Cars}         &86.51       & {\bf (1)} & 85.02       & (0)       & {\bf 86.57} & {\bf (1)} & 86.48       & {\bf (1)} & 86.48       &           \\
{\tt Cars-Ext}     &88.27       & {\bf (1) }& 88.27       & (0)       & {\bf 88.32} & {\bf (1)} & 88.26       & {\bf (1)} & 88.26       &           \\
{\tt Contracep}    &43.83       & (0)       & {\bf 44.12} & {\bf (2)} & 43.87       & (0)       & 43.63       & (0)       & 43.69       &           \\
{\tt Contracep-Ext}&43.39       & (0)       & {\bf 43.81} & {\bf (2)} & 43.76       & {\bf (2)} & 43.21       & (0)       & 43.32       &           \\
{\tt CoverType}    &54.1        & (0)       & 54.1        & (0)       & 54.1        & (0)       & 54.1        & (0)       & 54.1        & (0)       \\
{\tt CoverType-Ext}&54.1        & (0)       & 54.1        & (0)       & 54.1        & (0)       & 54.1        & (0)       & 54.1        & (0)       \\
{\tt Poker}        &{\bf 51.05} & {\bf (2)} & 50.56       & (0)       & 50.91       & (1)       & 50.8        & (1)       & 50.79       &           \\  
{\tt Shelter-15}   & 48.03      & (0)       & 48.03       & (0)       & {\bf 48.22} & {\bf (2)} & 48.22       & {\bf (2)} &             &           \\   
{\tt S.F. Crime-15}&{\bf 21.59} & {\bf (3)} & 20.53       & (0)       & 21.49       & (1)       & 21.52       & (1)       & 21.52       &           \\ 
{\tt Phonemes}     & 23.12      & (2)       & 22.52       & (1)       & {\bf 23.8 } & {\bf (3)} & 21.3        & (0)       & 22.11       &           \\ 
\hline
Average (Sum)      & 61.44      & (15)      & 61.77       & (12)      & 61.75       & (21)      & 61.15       & (8)       &             &    
       \end{tabular}
    \caption{Average accuracy and statistical tests  for  Conditional Inference trees 
with depth at most 5 using only nominal attributes. The best accuracy for each dataset is bold-faced.}
\label{tab:ctree-5}
\end{table*}


Table \ref{exp:numeric-5} shows experiments  similar to those presented at Table \ref{tab:nominal-5}, but now
using also the numeric attributes. We observed a significant gain in terms of accuracy for all datasets except for KDD98-2. 
Also note that GLSG was much inferior to the other criteria. Other than that, the comparison results were very similar to the ones found in table \ref{tab:nominal-5}.

\begin{table}
\small
\centering
\begin{tabular}{c|cc|cc|cc|cc|cc} 
Dataset            &\multicolumn{2}{c|}{Twoing} & \multicolumn{2}{c|}{GLSG} & \multicolumn{2}{c|}{GL$\chi^2$} & \multicolumn{2}{c|}{PC-ext}& \multicolumn{2}{c}{HcC}\\  \hline   
{\tt Adult}        & 84.2           & (1)       & 82.4       & (0)          &  84.2       & (1)               & {\bf 84.4 } & {\bf (3) }  & {\bf 84.4 } &           \\
{\tt KDD98}-2      & 80.9           & (0)       & 81.9       & {\bf (1)}    & {\bf 81.9 } & {\bf (1) }        & 81.9        & {\bf (1) }  & {\bf 81.9 } &           \\ 
%{\tt KDD98}-3      & {\bf 69.5 }    &{\bf  (2)} & 67.9       & (0)          & 69.1        & (1)               &             &             &             &           \\ 
%{\tt KDD98}-5      & 54.6           &  (0)      & 54.5       &  (0)         & {\bf 55.3 } & {\bf  (2) }       &             &             &             &           \\ 
{\tt KDD98}-9      & 45.7           & (1)       & 45.3       &  (0)         & {\bf 48.7 } & {\bf (3) }        &  47.4       & (2)         & 47.0        &           \\ 
{\tt Contracep}    & 54.1           & (1)       & 52.9       &  (0)         & 54          & (1)               & {\bf 55.1 } & {\bf (3) }  & 55.0        &           \\  
{\tt Contracep-Ext}& 51.7           & (0)       & 52.3       &  (0)         & 53.3        & {\bf (2) }        & {\bf 53.3 } & {\bf (2) }  & 52.8        &           \\ 
{\tt CoverType}    &  {\bf 70.3 }   & {\bf (2)} & 68.5       &  (0)         & 69.2        & (1)               &  70.2       & {\bf (2) }  & 70.2        &           \\  
{\tt CoverType-Ext}& 70.9           & (2)       & 70.3       &  (1)         & 69.2        & (0)               & {\bf 71.7 } & {\bf (3) }  & 71.2        &           \\ 
{\tt Shelter-15}   & 53.7           & (1)       & 51.9       &  (0)         & 54.6        & (2)               & {\bf 54.8 } & {\bf (3) }  & 54.6        &           \\   
{\tt S.F. Crime-15}& 23.5           & (2)       & 23.2       &  (0)         & {\bf 23.6 } & {\bf (3) }        & 23.5        & (1)         & 23.5        &           \\ 
\hline
Average (Sum)      & 59.44          & (10)      & 58.74      &  (2)         & 59.86       & (14)              & 60.3        & (20)        & 60.1        & 

\end{tabular}
\caption{Average accuracy and statistical test results for  Decision Trees using both nominal and numeric attributes.}
\label{exp:numeric-5}
\normalsize
\end{table}


\subsection{Maximum Depth 16}


In this subsection we explore the same experiments studied in the previous one, except now the maximum depth allowed is 16. This larger depth is common in most random forest methods, thus the importance of this analysis.

Table \ref{tab:nominal-16} presents the results of an experiment to
compare the accuracy of  Decision Trees built by our GL-methods with those built by Twoing, Hypercube Cover and PC-ext, using just the nominal attributes of the datasets. Once again PC-ext had the best results, but this time by a smaller margin. This suggests the possibility that, for a larger depth, it might not have the best results. Moreover, there was a balance between Twoing, Hypercube Cover and GL$\chi^2$, with GLSG being significantly worse. This suggests that GLSG loses competitiveness as the tree depth increases.

Table \ref{tab:ctree-16} presents the 
comparison between the different methods in the scenario where we use $c_{quad}$ to select the attribute at each node of the tree, and the criteria are used only for splitting the chosen attribute. We observed an advantage for GL$\chi^2$, followed by Twoing and Hypercube Cover. Both PC-ext and GLSG had significantly worse results. Once again it was not possible to run $c_{quad}$ in reasonable time for the {\tt Shelter-15} dataset.

\begin{table}
\small
\centering
\begin{tabular}{c|cc|cc|cc|cc|cc} 
Dataset             & \multicolumn{2}{c|}{Twoing} &  \multicolumn{2}{c|}{GLSG}  & \multicolumn{2}{c|}{GL$\chi^2$} & \multicolumn{2}{c|}{PC-ext}& \multicolumn{2}{c}{HcC}\\ \hline
{\tt Adult}         &  {\bf 82.52} & {\bf (2)}    &  82.38       &  (0)         &  82.43       & (0)              & 82.51      & {\bf (2)}    & {\bf 82.52}&            \\
{\tt Mushroom}      &  {\bf 100}   & {\bf (1)}    &  100         &  (0)         &  99.99       & (0)              & {\bf 100 } & {\bf (1)}    & {\bf 100}  &            \\
{\tt KDD98}-2       &  79.41       & (0)          &  {\bf 80.65} & {\bf (3)}    &  79.73       & (0)              & 79.91      & (1)          & 79.41      &            \\
%{\tt KDD98}-3       &  62.61       & (0)          &  63.33       & {\bf (1)}    &  {\bf 63.56} & {\bf (1)}        &            &              &            &            \\
%{\tt KDD98}-5       &  46.46       & (0)          &  46.39       & (0)          &  {\bf 47.53} & {\bf (2)}        &            &              &            &            \\
{\tt KDD98}-9       &  38.54       & (1)          &  37.97       & (0)          &  {\bf 39.68} & {\bf (3)}        & 38.55      & (1)          & 37.95      &            \\
{\tt Nursery}       &  93.51       & (2)          &  92.39       & (0)          &  {\bf 93.74} & {\bf (3)}        & 93.5       & (1)          & 93.5       &            \\
{\tt Nursery-ext}   &  95.83       & (1)          &  94.79       & (0)          &  {\bf 96.02} & {\bf (3)}        & 95.83      & (1)          & 95.83      &            \\
{\tt Cars}          &  92.82       & {\bf (1)}    &  90.51       & (0)          &  {\bf 92.88} & {\bf (1)}        & 92.69      & {\bf (1)}    & 92.69      &            \\
{\tt Cars-ext}      &  96.49       & {\bf (2)}    &  90.89       & (0)          &  94.44       & (1)              & 96.46      & {\bf (2)}    & {\bf 96.5} &            \\
{\tt Contracep}     &  43.5        & (0)          &  43.83       & (0)          &  {\bf 43.85} & {\bf (1)}        & 43.53      & (0)          & 43.53      &            \\
{\tt Contracep-ext} &  43.15       & (0)          &  {\bf 44.32} & {\bf (2)}    &  43.72       & (0)              & 43.75      & (1)          & 43.37      &            \\
{\tt CoverType}     &  64.09       & (1)          &  64.59       & (2)          &  63.61       & (0)              &{\bf 64.66} & {\bf (3)}    & {\bf 64.66}&            \\
{\tt CoverType-ext} &  {\bf 65.08} & {\bf (1)}    &  {\bf 65.08} & {\bf (1)}    &  65.05       & (0)              & 65.08      & {\bf (1)}    & {\bf 65.08}&            \\
{\tt Poker}         &  {\bf 52.24} & {\bf (2)}    &  49.88       & (0)          &  51.83       & (1)              & 52.09      & {\bf (2)}    & 52.09      &            \\
{\tt Shelter-15}    &  47.64       & (1)          &  46.52       & (0)          & {\bf 48.09}  & {\bf (3)}        & 47.71      & (1)          & 47.26      &            \\
{\tt S.F. Crime-15} &  22.08       & {\bf (1)}    &  22.06       & (0)          & {\bf 22.08}  & {\bf (1)}        & 22.08      & {\bf (1)}    & 22.08      &            \\
{\tt Phonemes}      &  37.13       & (2)          &  35.9        & (0)          & 35.75        & (0)              &{\bf 37.95} & {\bf (3)}    & 37.89      &            \\
\hline
Average (Sum)       &  65.88       & (18)         &  65.11       & (8)          & 65.81        & (17)             &  66.02     & (22)         & 65.9       &    
\end{tabular}
\normalsize
\caption{Average accuracy and statistical tests  for  decision trees 
with depth at most 16 using only nominal attributes. The best accuracy for each dataset is bold-faced, even when multiple criteria have the same accuracy in the table because of rounding.}
\label{tab:nominal-16}
\end{table}

    \begin{table}
    \small
      \centering

\begin{tabular}{c|cc|cc|cc|cc|cc} 
Dataset             & \multicolumn{2}{c|}{CI-Twoing} &   \multicolumn{2}{c|}{CI-GLSG} & \multicolumn{2}{c|}{CI-GL$\chi^2$} & \multicolumn{2}{c|}{CI-PC-ext}& \multicolumn{2}{c}{CI-HcC}\\  \hline   
{\tt Adult}         & 82.33      &  {\bf (2)}        &   82.18      & (0)             & {\bf 82.35} &  {\bf (2)}           & 82.28       & (1)             & 82.33       &             \\
{\tt Mushroom}      & 99.55      &  (1)              &   99.6       & (1)             & {\bf 99.64} &  {\bf (2)}           & 99.4        & (0)             & 99.55       &             \\
{\tt KDD98}-2       & 79.92      &  (1)              &  {\bf 81.53} & {\bf (3)}       &  80.65      &  (2)                 & 79.72       & (0)             & 79.92       &             \\
%{\tt KDD98}-3       &63.72       &  (0)              &  {\bf 64.66} & {\bf (2)}       &  63.56      &  (0)                 &             &                 &             &             \\
%{\tt KDD98}-5       &46.88       &  (0)              &  {\bf 48}    & {\bf (1)}       &  47.75      &  {\bf (1)}           &             &                 &             &            \\
{\tt KDD98}-9       & 39.44      &  (1)              &  {\bf 40.65} & {\bf (3)}       &  40.01      &  (2)                 & 38.85       & (0)             & 39.07       &             \\
{\tt Nursery}       & 93.55      &  {\bf (1)}        &   92.22      & (0)             & {\bf 93.56} &  {\bf (1)}           & 93.55       & {\bf (1)}       & 93.55       &             \\
{\tt Nursery-ext}   & 93.64      &  {\bf (1)}        &   92.32      & (0)             & {\bf 93.65} &  {\bf (1)}           & 93.64       & {\bf (1)}       & 93.64       &             \\
{\tt Cars}          & 92.74      &  (1)              &   87.19      & (0)             & {\bf 92.92} &  {\bf (3)}           & 92.67       & (1)             & 92.67       &             \\
{\tt Cars-ext}      & 93.12      &  (1)              &   87.38      & (0)             & {\bf 93.34} &  {\bf (3)}           & 93.08       & (1)             & 93.08       &             \\
{\tt Contracep}     & 43.82      &  {\bf (1)}        &   {\bf 44.05}& {\bf (1)}       &  43.81      &  (0)                 & 43.59       & (0)             & 43.6        &             \\
{\tt Contracep-ext} & 43.21      &  (0)              &   {\bf 43.78}& {\bf (2)}       &  43.72      &  {\bf (2)}           & 43.07       & (0)             & 43.12       &             \\
{\tt CoverType}     & 61.88      &  (0)              &   61.88      & (0)             &  61.88      &  (0)                 & 61.88       & (0)             & 61.88       & (0)         \\
{\tt CoverType-ext} & 61.88      &  (0)              &   61.88      & (0)             &  61.88      &  (0)                 & 61.88       & (0)             & 61.88       & (0)         \\
{\tt Poker}         & {\bf 51.4} &  {\bf (3)}        &   50.67      & (0)             &  50.84      &  (1)                 & 51.04       & (1)             & 51.03       &             \\
{\tt Shelter-15}    & 47.66      &  (1)              &   47.32      & (0)             & {\bf 48.14} &  {\bf (3)}           & 47.82       & (2)             &             &             \\
{\tt S.F. Crime-15} & 22.07      &  (0)              &   22.07      & (0)             & {\bf 22.07} &  {\bf (1)}           & 22.07       & (0)             & {\bf 22.07} &             \\
{\tt Phonemes}      & 33.91      &  {\bf (2)}        &   31.26      & (0)             &  33.32      &  (1)                 & 34.08       & {\bf (2)}       & {\bf 34.77} &             \\
\hline
Average (Sum)       & 66.16      &  (16)             &   65.24      & (10)            &  66.24      &  (24)                & 64.91       & (10)            &             &
       \end{tabular}
        \caption{Average accuracy and statistical tests  for  conditional inference trees 
with depth at most 16 using only nominal attributes. The best accuracy for each dataset is bold-faced, even when multiple criteria have the same accuracy in the table because of rounding.}
\label{tab:ctree-16}
\normalsize
\end{table}


Table \ref{tab:time-16} shows the running time of each criteria when used for both selecting and splitting purposes (the experiment of Table \ref{tab:nominal-16}).
When the number of classes is small all the criteria have very similar execution time. As the number of classes increases, both PC-ext and the GL-based methods become much faster than Twoing and Hypercube Cover, with the turning point being once again around $k=7$. For datasets with 15 classes our criteria are 30-400 times faster than Twoing, while PC-ext is 50-600 times faster. We also ran experiments using all the classes available in both the {\tt S.F. Crime} and {\tt Shelter} datasets (39 and 22, respectively). Twoing cannot be executed in a reasonable time with that many classes, while GLSG and GL$\chi^2$ ran in approximately 100 seconds on the {\tt S.F. Crime} dataset and  300 seconds on the {\tt Shelter} dataset (PC-ext ran in 110 and 33 seconds, respectively). Since the execution time for our criteria in this experiment grew onde again in an approximately linear fashion with $k$, it suggests that they can also be used with datasets that have a much larger number of classes. It is also interesting to note that the aggregated attributes usually appeared at or near the root of the decision trees.

\begin{table}[]
\small
\centering
\begin{tabular}{c|c|c|c|c|c|c}
Dataset             & k  & Twoing        & GLSG      & GL$\chi^2$  & PC-ext    & HcC     \\ \hline
{\tt Adult}         & 2  & 4.3           & 4.3       & 7.1         &{\bf 4}    & 5.8     \\
{\tt Mushroom}      & 2  & 0.7           & {\bf 0.6} & 0.9         & 0.6       & 1       \\
{\tt KDD98}-2       & 2  & 10.8          & 57.8      & 60.8        &{\bf 8.3}  & 10.4    \\
{\tt Contracep}     & 3  & 0.2           & 0.2       & 0.1         &{\bf 0.1}  & 0.1     \\
{\tt Contracep-Ext} & 3  & 0.2           & 0.3       & 0.3         &{\bf 0.2}  & 0.3     \\
%{\tt KDD98}-3       & 3  & {\bf 12.5}    & 73.3      & 82.4        &           &        \\
{\tt Cars}          & 4  & 0.3           & 0.2       & 0.2         &{\bf 0.2}  & 0.3    \\
{\tt Cars-Ext}      & 4  & 0.3           & 0.4       & 0.4         &{\bf 0.2}  & 0.3    \\
{\tt Nursery}       & 5  & 1.6           & 1.4       & 1.4         &{\bf 1.3}  & 1.9    \\
{\tt Nursery-Ext}   & 5  & 1.7           & 3.9       & 3.6         &{\bf 1.4}  & 2.3    \\
%{\tt KDD98}-5       & 5  & {\bf 22.2}    & 100.8     & 82.6        &           &        \\
{\tt CoverType}     & 7  & 846.8         &{\bf 373.4}& 969.6       & 472.4     & 669    \\
{\tt CoverType-Ext} & 7  & 338.6         & 280.5     & 505.7       &{\bf 200.4}& 296    \\
{\tt KDD98}-9       & 9  & 209.9         & 119.9     & 112.5       &{\bf 23.8} & 443    \\
{\tt Poker}         & 10 & 10.7          & 3.9       & {\bf 3.7}   & 3.8       & 18     \\
{\tt Shelter-15}    & 15 & 5183.3        & 155       & 165.7       &{\bf 18.6} & 7311.6 \\
{\tt S.F. Crime-15} & 15 & 2667.9        & 94.2      & 79.6        &{\bf 50.5} & 3188.9 \\
{\tt Phonemes}      & 15 & 3738.6        & 8.7       & 10.2        &{\bf 6.3}  & 5804.8
\end{tabular}
\caption{Average time in seconds of a 3-fold cross validation
for building decision trees with depth at most 16.
The fastest method for each dataset is bold-faced.}
\label{tab:time-16}
\end{table}


Table \ref{exp:numeric-16} shows experiments  similar to those presented at Table \ref{tab:nominal-16}, except now it also uses the numeric attributes. We observed a significant gain in terms of accuracy for almost all datasets. 
The performance of GL$\chi^2$ was competitive competitive or better than the other criteria in almost all datasets, except for both CoverType, where curiously GLSG had the best results.

\begin{table}
\small
\centering
\begin{tabular}{c|cc|cc|cc|cc|cc} 
Dataset              &        \multicolumn{2}{c|}{Twoing} &   \multicolumn{2}{c|}{GLSG} &   \multicolumn{2}{c|}{GL$\chi^2$} & \multicolumn{2}{c|}{PC-ext}  & \multicolumn{2}{c}{HcC}  \\  \hline   
{\tt Adult}          &  83            &  (1)              &  77.34      &  (0)          &  83.21       &  {\bf (2)}         & {\bf 83.28} & {\bf (2)}     & 83.25 &  \\
{\tt KDD98}-2        &  76.67         &  (2)              &  76.36      &  (1)          &  76.04       &  (0)               & {\bf 77.87} & {\bf (3)}     & 77.14 &  \\
%{\tt KDD98}-3        &  63.24         &  (1)              &  62.21      &  (0)          &  {\bf 63.94} &  {\bf (2)}         &             &              &        &  \\
%{\tt KDD98}-5        &  47.5          &  (1)              &  46.35      &  (0)          &  {\bf 49.71} &  {\bf (2)}         &             &              &        &  \\
{\tt KDD98}-9        &  37.73         &  (1)              &  37.49      &  (0)          &  {\bf 43.44} &  {\bf (3)}         &  39.88      & (2)           & 38.96  &   \\
{\tt Contracep}      &  48.78         &  {\bf (1)}        &  48.01      &  (0)          &  48.66       &  {\bf (1)}         & {\bf 48.93} & {\bf (1)}     & 48.86  &   \\
{\tt Contracep-Ext}  &  48.53         &  (0)              &  48.15      &  (0)          &  48.6        &  (0)               &  48.52      & (0)           & {\bf 49.31 } &    \\
{\tt CoverType}      &  85.14         &  (1)              &  {\bf 90.32}&  {\bf (3)}    &  81.38       &  (0)               &  86.23      & (2)           & 86.23 &   \\
{\tt CoverType-Ext}  &  89.1          &  (2)              &  {\bf 92.03}&  {\bf (3)}    &  82.46       &  (0)               &  88.32      & (1)           & 89.39 &    \\
{\tt Shelter-15}     &  53.79         &  (1)              &  52         &  (0)          &  {\bf 54.4}  &  {\bf (3)}         &  53.82      & (1)           &       &   \\   
{\tt S.F. Crime-15}  &  26.66         &  (0)              &  26.71      &  (1)          &  {\bf 27.13} &  {\bf (2)}         &  27.13      & {\bf (2)}     &       &   \\
\hline
Average (Sum)        &   61.04        &  (8)              & 60.93       &  (8)          &   60.59      &  (11)              &  61.55      & (14)          &       & 

\end{tabular}
\caption{Average accuracy and statistical test results for  Decision Trees using both nominal and numeric attributes with depth at most 16.}
\label{exp:numeric-16}
\normalsize
\end{table}

